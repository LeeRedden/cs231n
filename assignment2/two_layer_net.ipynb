{
 "metadata": {
  "name": "",
  "signature": "sha256:da5513746cbd8f3b007ba5f893bfebd9e2a95b5ac76c24049a20ecfdd7fd531e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing a Neural Network\n",
      "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A bit of setup\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
      "plt.rcParams['image.interpolation'] = 'nearest'\n",
      "plt.rcParams['image.cmap'] = 'gray'\n",
      "\n",
      "# for auto-reloading external modules\n",
      "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "def rel_error(x, y):\n",
      "  \"\"\" returns relative error \"\"\"\n",
      "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The neural network parameters will be stored in a dictionary (`model` below), where the keys are the parameter names and the values are numpy arrays. Below, we initialize toy data and a toy model that we will use to verify your implementations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create some toy data to check your implementations\n",
      "input_size = 4\n",
      "hidden_size = 10\n",
      "num_classes = 3\n",
      "num_inputs = 5\n",
      "\n",
      "def init_toy_model():\n",
      "  model = {}\n",
      "  model['W1'] = np.linspace(-0.2, 0.6, num=input_size*hidden_size).reshape(input_size, hidden_size)\n",
      "  model['b1'] = np.linspace(-0.3, 0.7, num=hidden_size)\n",
      "  model['W2'] = np.linspace(-0.4, 0.1, num=hidden_size*num_classes).reshape(hidden_size, num_classes)\n",
      "  model['b2'] = np.linspace(-0.5, 0.9, num=num_classes)\n",
      "  return model\n",
      "\n",
      "def init_toy_data():\n",
      "  X = np.linspace(-0.2, 0.5, num=num_inputs*input_size).reshape(num_inputs, input_size)\n",
      "  y = np.array([0, 1, 2, 2, 1])\n",
      "  return X, y\n",
      "\n",
      "model = init_toy_model()\n",
      "X, y = init_toy_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Forward pass: compute scores\n",
      "Open the file `cs231n/classifiers/neural_net.py` and look at the function `two_layer_net`. This function is very similar to the loss functions you have written for the SVM and Softmax exercises: It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. \n",
      "\n",
      "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cs231n.classifiers.neural_net import two_layer_net\n",
      "\n",
      "scores = two_layer_net(X, model)\n",
      "print scores\n",
      "correct_scores = [[-0.5328368, 0.20031504, 0.93346689],\n",
      " [-0.59412164, 0.15498488, 0.9040914 ],\n",
      " [-0.67658362, 0.08978957, 0.85616275],\n",
      " [-0.77092643, 0.01339997, 0.79772637],\n",
      " [-0.89110401, -0.08754544, 0.71601312]]\n",
      "\n",
      "# the difference should be very small. We get 3e-8\n",
      "print 'Difference between your scores and correct scores:'\n",
      "print np.sum(np.abs(scores - correct_scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.5328368   0.20031504  0.93346689]\n",
        " [-0.59412164  0.15498488  0.9040914 ]\n",
        " [-0.67658362  0.08978957  0.85616275]\n",
        " [-0.77092643  0.01339997  0.79772637]\n",
        " [-0.89110401 -0.08754544  0.71601312]]\n",
        "Difference between your scores and correct scores:\n",
        "3.84868230029e-08\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Forward pass: compute loss\n",
      "In the same function, implement the second part that computes the data and regularizaion loss."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reg = 0.1\n",
      "loss, _ = two_layer_net(X, model, y, reg)\n",
      "correct_loss = 1.38191946092\n",
      "\n",
      "# should be very small, we get 5e-12\n",
      "print 'Difference between your loss and correct loss:'\n",
      "print np.sum(np.abs(loss - correct_loss))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.5328368   0.20031504  0.93346689]\n",
        " [-0.59412164  0.15498488  0.9040914 ]\n",
        " [-0.67658362  0.08978957  0.85616275]\n",
        " [-0.77092643  0.01339997  0.79772637]\n",
        " [-0.89110401 -0.08754544  0.71601312]]\n",
        "Difference between your loss and correct loss:\n",
        "4.67692551354e-12\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Backward pass\n",
      "Implement the rest of the function. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cs231n.gradient_check import eval_numerical_gradient\n",
      "\n",
      "# Use numeric gradient checking to check your implementation of the backward pass.\n",
      "# If your implementation is correct, the difference between the numeric and\n",
      "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
      "\n",
      "loss, grads = two_layer_net(X, model, y, reg)\n",
      "\n",
      "# these should all be less than 1e-8 or so\n",
      "for param_name in grads:\n",
      "  param_grad_num = eval_numerical_gradient(lambda W: two_layer_net(X, model, y, reg)[0], model[param_name], verbose=False)\n",
      "  print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "b2 max relative error: 4.404011e-11\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "b1 max relative error: 2.027453e-08\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "W1 max relative error: 4.426512e-09\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "[ 0.2  0.2  0.2  0.2  0.2]\n",
        "[[-0.2  0.   0. ]\n",
        " [ 0.  -0.2  0. ]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.   0.  -0.2]\n",
        " [ 0.  -0.2  0. ]]\n",
        "W2 max relative error: 3.379923e-09\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Train the network\n",
      "To train the network we will use SGD with Momentum. Last assignment you implemented vanilla SGD. You will now implement the momentum update and the RMSProp update. Open the file `classifier_trainer.py` and familiarze yourself with the `ClassifierTrainer` class. It performs optimization given an arbitrary cost function data, and model. By default it uses vanilla SGD, which we have already implemented for you. First, run the optimization below using Vanilla SGD:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cs231n.classifier_trainer import ClassifierTrainer\n",
      "\n",
      "model = init_toy_model()\n",
      "trainer = ClassifierTrainer()\n",
      "# call the trainer to optimize the loss\n",
      "# Notice that we're using sample_batches=False, so we're performing Gradient Descent (no sampled batches of data)\n",
      "best_model, loss_history, _, _ = trainer.train(X, y, X, y,\n",
      "                                             model, two_layer_net,\n",
      "                                             reg=0.001,\n",
      "                                             learning_rate=1e-1, momentum=0.0, learning_rate_decay=1,\n",
      "                                             update='sgd', sample_batches=False,\n",
      "                                             num_epochs=120,\n",
      "                                             verbose=False)\n",
      "print 'Final loss with vanilla SGD: %f' % (loss_history[-1], )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "starting iteration  0\n",
        "starting iteration  10\n",
        "starting iteration  20\n",
        "starting iteration  30\n",
        "starting iteration  40\n",
        "starting iteration  50\n",
        "starting iteration  60\n",
        "starting iteration  70\n",
        "starting iteration  80\n",
        "starting iteration  90\n",
        "starting iteration  100\n",
        "starting iteration  110\n",
        "Final loss with vanilla SGD: 0.969283\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now fill in the **momentum update** in the first missing code block inside the `train` function, and run the same optimization as above but with the momentum update. You should see a much better result in the final obtained loss:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = init_toy_model()\n",
      "trainer = ClassifierTrainer()\n",
      "# call the trainer to optimize the loss\n",
      "# Notice that we're using sample_batches=False, so we're performing Gradient Descent (no sampled batches of data)\n",
      "best_model, loss_history, _, _ = trainer.train(X, y, X, y,\n",
      "                                             model, two_layer_net,\n",
      "                                             reg=0.001,\n",
      "                                             learning_rate=1e-1, momentum=0.9, learning_rate_decay=1,\n",
      "                                             update='momentum', sample_batches=False,\n",
      "                                             num_epochs=100,\n",
      "                                             verbose=False)\n",
      "correct_loss = 0.494394\n",
      "print 'Final loss with momentum SGD: %f. We get: %f' % (loss_history[-1], correct_loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now also implement the **RMSProp** update rule inside the `train` function and rerun the optimization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = init_toy_model()\n",
      "trainer = ClassifierTrainer()\n",
      "# call the trainer to optimize the loss\n",
      "# Notice that we're using sample_batches=False, so we're performing Gradient Descent (no sampled batches of data)\n",
      "best_model, loss_history, _, _ = trainer.train(X, y, X, y,\n",
      "                                             model, two_layer_net,\n",
      "                                             reg=0.001,\n",
      "                                             learning_rate=1e-1, momentum=0.9, learning_rate_decay=1,\n",
      "                                             update='rmsprop', sample_batches=False,\n",
      "                                             num_epochs=100,\n",
      "                                             verbose=False)\n",
      "correct_loss = 0.439368\n",
      "print 'Final loss with RMSProp: %f. We get: %f' % (loss_history[-1], correct_loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Load the data\n",
      "Now that you have implemented a two-layer network that passes gradient checks, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cs231n.data_utils import load_CIFAR10\n",
      "\n",
      "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
      "    \"\"\"\n",
      "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
      "    it for the two-layer neural net classifier. These are the same steps as\n",
      "    we used for the SVM, but condensed to a single function.  \n",
      "    \"\"\"\n",
      "    # Load the raw CIFAR-10 data\n",
      "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
      "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
      "        \n",
      "    # Subsample the data\n",
      "    mask = range(num_training, num_training + num_validation)\n",
      "    X_val = X_train[mask]\n",
      "    y_val = y_train[mask]\n",
      "    mask = range(num_training)\n",
      "    X_train = X_train[mask]\n",
      "    y_train = y_train[mask]\n",
      "    mask = range(num_test)\n",
      "    X_test = X_test[mask]\n",
      "    y_test = y_test[mask]\n",
      "\n",
      "    # Normalize the data: subtract the mean image\n",
      "    mean_image = np.mean(X_train, axis=0)\n",
      "    X_train -= mean_image\n",
      "    X_val -= mean_image\n",
      "    X_test -= mean_image\n",
      "\n",
      "    # Reshape data to rows\n",
      "    X_train = X_train.reshape(num_training, -1)\n",
      "    X_val = X_val.reshape(num_validation, -1)\n",
      "    X_test = X_test.reshape(num_test, -1)\n",
      "\n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
      "\n",
      "\n",
      "# Invoke the above function to get our data.\n",
      "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
      "print 'Train data shape: ', X_train.shape\n",
      "print 'Train labels shape: ', y_train.shape\n",
      "print 'Validation data shape: ', X_val.shape\n",
      "print 'Validation labels shape: ', y_val.shape\n",
      "print 'Test data shape: ', X_test.shape\n",
      "print 'Test labels shape: ', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Train a network\n",
      "To train our network we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cs231n.classifiers.neural_net import init_two_layer_model\n",
      "\n",
      "model = init_two_layer_model(32*32*3, 50, 10) # input size, hidden size, number of classes\n",
      "trainer = ClassifierTrainer()\n",
      "best_model, loss_history, train_acc, val_acc = trainer.train(X_train, y_train, X_val, y_val,\n",
      "                                             model, two_layer_net,\n",
      "                                             num_epochs=5, reg=1.0,\n",
      "                                             momentum=0.9, learning_rate_decay = 0.95,\n",
      "                                             learning_rate=1e-5, verbose=True)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Debug the training\n",
      "With the default parameters we provided above, you should get a validation accuracy of about 0.37 on the validation set. This isn't very good.\n",
      "\n",
      "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
      "\n",
      "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the loss function and train / validation accuracies\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.plot(loss_history)\n",
      "plt.title('Loss history')\n",
      "plt.xlabel('Iteration')\n",
      "plt.ylabel('Loss')\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.plot(train_acc)\n",
      "plt.plot(val_acc)\n",
      "plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right')\n",
      "plt.xlabel('Epoch')\n",
      "plt.ylabel('Clasification accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'train_acc' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-c8d464992bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'train_acc' is not defined"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAHpCAYAAAA/AGDTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7faf736f4cd0>"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cs231n.vis_utils import visualize_grid\n",
      "\n",
      "# Visualize the weights of the network\n",
      "\n",
      "def show_net_weights(model):\n",
      "    plt.imshow(visualize_grid(model['W1'].T.reshape(-1, 32, 32, 3), padding=3).astype('uint8'))\n",
      "    plt.gca().axis('off')\n",
      "    plt.show()\n",
      "\n",
      "show_net_weights(model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tune your hyperparameters\n",
      "\n",
      "**What's wrong?**. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
      "\n",
      "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the momentum and learning rate decay parameters, but you should be able to get good performance using the default values.\n",
      "\n",
      "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 50% on the validation set. Our best network gets over 56% on the validation set.\n",
      "\n",
      "**Experiment**: You goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. For every 1% above 56% on the Test set we will award you with one extra bonus point. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best_model = None # store the best model into this \n",
      "\n",
      "#################################################################################\n",
      "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
      "# model in best_model.                                                          #\n",
      "#                                                                               #\n",
      "# To help debug your network, it may help to use visualizations similar to the  #\n",
      "# ones we used above; these visualizations will have significant qualitative    #\n",
      "# differences from the ones we saw above for the poorly tuned network.          #\n",
      "#                                                                               #\n",
      "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
      "# write code to sweep through possible combinations of hyperparameters          #\n",
      "# automatically like we did on the previous assignment.                         #\n",
      "#################################################################################\n",
      "pass\n",
      "#################################################################################\n",
      "#                               END OF YOUR CODE                                #\n",
      "#################################################################################"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the weights\n",
      "show_net_weights(best_model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Run on the test set\n",
      "When you are done experimenting, you should evaluate your final trained network on the test set. \n",
      "\n",
      "**We will give you extra bonus point for every 1% of accuracy above 56%.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores_test = two_layer_net(X_test, best_model)\n",
      "print 'Test accuracy: ', np.mean(np.argmax(scores_test, axis=1) == y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}